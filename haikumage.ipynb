{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Install & import required packages<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e889c31",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-06T19:44:57.544546Z",
     "iopub.status.busy": "2023-10-06T19:44:57.543912Z",
     "iopub.status.idle": "2023-10-06T19:44:57.994288Z",
     "shell.execute_reply": "2023-10-06T19:44:57.993085Z"
    },
    "papermill": {
     "duration": 0.457519,
     "end_time": "2023-10-06T19:44:57.997181",
     "exception": false,
     "start_time": "2023-10-06T19:44:57.539662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install transformers torch\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM, AutoModelForVision2Seq, AutoTokenizer, AutoModelForCausalLM, pipeline, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GenerationConfig, T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk import word_tokenize\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5171a",
   "metadata": {},
   "source": [
    "<h2>Load models<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2word model (kosmos) from https://huggingface.co/microsoft/led-base-16384\n",
    "kosmos_name = \"microsoft/kosmos-2-patch14-224\"                    \n",
    "kosmos_model = AutoModelForVision2Seq.from_pretrained(kosmos_name)\n",
    "kosmos_processor = AutoProcessor.from_pretrained(kosmos_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load word to haiku model\n",
    "model_name = \"fabianmmueller/deep-haiku-gpt-2\"\n",
    "\n",
    "syllable_tokenizer = SyllableTokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c54515",
   "metadata": {},
   "source": [
    "<h2>Image2Word</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<grounding>An image of\"\n",
    "\n",
    "# User inputted image\n",
    "image = Image.open(\"data/snowman.jpg\")\n",
    "\n",
    "inputs = kosmos_processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = kosmos_model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "generated_text = kosmos_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = kosmos_processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "# print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = kosmos_processor.post_process_generation(generated_text)\n",
    "\n",
    "kosmos_output = entities[0][0]\n",
    "print(kosmos_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b4a386",
   "metadata": {},
   "source": [
    "<h2>Word2Haiku</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d82450",
   "metadata": {},
   "source": [
    "<h3>Load Pretrained Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e12af",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5efbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load pretrained model\n",
    "\n",
    "pt_model_name = \"fabianmmueller/deep-haiku-gpt-2\"\n",
    "\n",
    "syllable_tokenizer = SyllableTokenizer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_model_name)\n",
    "pt_model = AutoModelForCausalLM.from_pretrained(pt_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bf1a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_model_name = \"Voicelab/vlt5-base-keywords\"\n",
    "keyword_model = T5ForConditionalGeneration.from_pretrained(keyword_model_name)\n",
    "keyword_tokenizer = T5Tokenizer.from_pretrained(keyword_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb602e9",
   "metadata": {},
   "source": [
    "<h3>Define Variables</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "batch_size = 64\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7facce",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78631f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for tokenizing data\n",
    "\n",
    "def preprocess_haikus(data):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(data['keywords'])):\n",
    "        keyw = \"( \" + data['keywords'][i] + \" = \"\n",
    "        inputs.append(keyw )\n",
    "        targets.append(keyw+ data[\"text\"][i] + \")\")\n",
    "    # inputs = [\"( \" + chunk + \" =\" for chunk in data[\"keywords\"]]\n",
    "    # targets = [sentence2syllables(chunk) for chunk in data[\"text\"]]\n",
    "    # targets = [(chunk) for chunk in data[\"text\"]]\n",
    "    return (tokenizer(inputs, text_target = targets, padding = 'max_length', truncation=True, max_length=max_length, return_tensors = 'pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up results from the pretrained model\n",
    "\n",
    "def clean_result(result):\n",
    "    result_text = result[0]['generated_text']\n",
    "    start = result_text.find(\"=\")\n",
    "    end = result_text.find(\")\")\n",
    "    start = start if start!=-1 else 0\n",
    "    end = end if end!=-1 else len(result_text)\n",
    "    return result_text[start+1:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adedd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to write to json file\n",
    "\n",
    "import json \n",
    "    \n",
    "# Convert and write JSON object to file\n",
    "def write_json(data, filename):\n",
    "    with open(filename, \"w\") as outfile: \n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fd71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to update json file as we preprocess dataset 2\n",
    "\n",
    "def update_json(new_data, filename='dataset2.json', key=\"keywords\"):\n",
    "    with open(filename,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[key].extend(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "# data is a batch\n",
    "# add a key \"syllables\" into the dictionary to have text tokenized into syllables\n",
    "def intoSyllables(data):\n",
    "    processed = [d.replace(' /', '') for d in data['text']]\n",
    "    sentences = [sentence.split(' ') for sentence in processed]\n",
    "    syllables = [[syllable_tokenizer.tokenize(word.lower()) for word in sentence] for sentence in sentences ]\n",
    "    for i in range(len(syllables)): \n",
    "        syllables[i] = ['syllables: '] + [item for sublist in syllables[i] for item in sublist]\n",
    "        data['keywords'][i] = \"keywords: \"+ data['keywords'][i]\n",
    "    data['syllables'] = syllables\n",
    "    return data\n",
    "\n",
    "def sentence2syllables(sentence):\n",
    "    # sentence = sentence.replace('/ ', '')\n",
    "    words = sentence.split(' ')\n",
    "    syllables = [syllable_tokenizer.tokenize(word) for word in words]\n",
    "    return ' '.join([item for sublist in syllables for item in sublist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading datasets</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset 1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load dataset 1 (same as what the pretrained model was trained on)\n",
    "\n",
    "haikus = load_dataset(\"statworx/haiku\")\n",
    "print(haikus)\n",
    "print(haikus['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting dataset 1 into train and test\n",
    "\n",
    "haikus = haikus[\"train\"].train_test_split(test_size=0.001)\n",
    "tokenized_haikus = {}\n",
    "tokenized_haikus['test'] = haikus['test'].map(preprocess_haikus, batched = True, batch_size=batch_size, remove_columns=['source', 'text_phonemes', 'keyword_phonemes', 'gruen_score', 'text_punc'])\n",
    "haikus = haikus['train'].train_test_split(test_size=0.1)\n",
    "tokenized_haikus['train'] = haikus['train'].map(preprocess_haikus, batched = True, batch_size=batch_size, remove_columns=['source', 'text_phonemes', 'keyword_phonemes', 'gruen_score', 'text_punc'])\n",
    "tokenized_haikus['validation'] = haikus['test'].map(preprocess_haikus, batched = True, batch_size=batch_size, remove_columns=['source', 'text_phonemes', 'keyword_phonemes', 'gruen_score', 'text_punc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_haikus)\n",
    "\n",
    "print(tokenized_haikus['train']['text'][0])\n",
    "print(tokenized_haikus['train']['labels'][0])\n",
    "print(tokenizer.decode(tokenized_haikus['train']['labels'][0]))\n",
    "\n",
    "print(tokenized_haikus['train']['keywords'][0])\n",
    "print(tokenized_haikus['train']['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_haikus['train']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset 2 ( + preprocessing)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read dataset from txt file into a dictionary\n",
    "### RUN ONCE ONLY\n",
    "\n",
    "# data2 = {'text': [],\n",
    "#          'keywords':[]}\n",
    "# f = open(\"dataset2.txt\", \"r\")\n",
    "# content = [ln for ln in f]\n",
    "# for line in range(0, len(content), 5):\n",
    "#     data2['text'].append(content[line].strip() + \". / \" +  content[line+1].strip() + \". / \" + content[line+2].strip() + \". \") \n",
    "# print(data2['text'][0])\n",
    "# f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess new trianing data from web \n",
    "### RUN ONCE ONLY\n",
    "\n",
    "# task_prefix = \"Keywords: \"\n",
    "# inputs = data2['text']\n",
    "# count = 0\n",
    "\n",
    "# write_json(data2, \"dataset2.json\")\n",
    "# total = 0\n",
    "\n",
    "# for sample in inputs:\n",
    "#     input_sequences = [task_prefix + sample]\n",
    "#     input_ids = keyword_tokenizer(\n",
    "#         input_sequences, return_tensors=\"pt\", truncation=True\n",
    "#     ).input_ids\n",
    "#     output = keyword_model.generate(input_ids, no_repeat_ngram_size=1, num_beams=5)\n",
    "#     predicted = keyword_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#     data2['keywords'].append(predicted.strip().split(',')[0])\n",
    "#     count+=1\n",
    "#     if (count==100):\n",
    "#         print(sample)\n",
    "#         print(data2['keywords'][-1])\n",
    "#         update_json(data2['keywords'], \"dataset2.json\")\n",
    "#         data2['keywords'] = []\n",
    "#         count = 0\n",
    "#         total += 1\n",
    "    \n",
    "\n",
    "# print(data2['keywords'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "web_data = load_dataset('json', data_files='web_haikus.json')\n",
    "dict = {\"text\": web_data['train']['text'][0][:len(web_data['train']['keywords'][0])], \"keywords\": web_data['train']['keywords'][0]}\n",
    "web_data = Dataset.from_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitting dataset 2 into train and test\n",
    "\n",
    "web_data = web_data.train_test_split(test_size = 0.1)\n",
    "tokenized_web_data = {}\n",
    "tokenized_web_data['train'] = web_data['train'].map(preprocess_haikus, batched = True, batch_size=batch_size)\n",
    "tokenized_web_data['test'] = web_data['test'].map(preprocess_haikus, batched = True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_web_data)\n",
    "\n",
    "print(tokenized_web_data['train']['text'][0])\n",
    "print(tokenized_web_data['train']['labels'][0])\n",
    "print(tokenizer.decode(tokenized_haikus['train']['labels'][0]))\n",
    "\n",
    "print(tokenized_web_data['train']['keywords'][0])\n",
    "print(tokenized_web_data['train']['input_ids'][0])\n",
    "print(tokenizer.decode(tokenized_haikus['train']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating on pretrained model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating sample outputs with the pretrained model\n",
    "\n",
    "prompt = \"( iced coffee = \"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=pt_model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "generator = pipeline('text-generation', model = pt_model_name)\n",
    "\n",
    "result = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning results from pretrained model & measure perplexity in comparison to gpt2\n",
    "\n",
    "cleaned_result = clean_result(result)\n",
    "ppl_pt = perplexity.compute(predictions=cleaned_result, model_id=pt_model_name)\n",
    "print(ppl_pt['mean_perplexity'])\n",
    "ppl_gpt_pt = perplexity.compute(predictions=cleaned_result, model_id=\"gpt2\")\n",
    "print(ppl_gpt_pt['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training on pretrained model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./result9\"\n",
    "# train_batch = 25\n",
    "# eval_batch= 16\n",
    "epochs = 2\n",
    "save_steps = 500\n",
    "learning_rate=0.01\n",
    "weight_decay=0.01\n",
    "save_total_limit=3\n",
    "logging_steps=200\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        weight_decay = weight_decay,\n",
    "        logging_steps = logging_steps,\n",
    "        # learning_rate = learning_rate,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size= batch_size,\n",
    "        per_device_eval_batch_size = batch_size,\n",
    "        save_steps=save_steps,\n",
    "        # fp16=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    pred_logits = torch.FloatTensor(eval_pred.predictions) # what transformer returns\n",
    "    # pred_labels = torch.FloatTensor(eval_pred.label_ids)\n",
    "    predictions = torch.argmax(pred_logits, -1)\n",
    "    # decoded_predictions = [tokenizer.decode(predictions) for sen in predictions]\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions)\n",
    "    print(decoded_predictions)\n",
    "    # decoded_labels = tokenizer.batch_decode(pred_labels, skip_special_tokens=True)\n",
    "    # print(decoded_labels)\n",
    "    # print(\"label: \", decoded_labels)\n",
    "    # print(\"predictions\", decoded_predictions)\n",
    "    return perplexity.compute(predictions=decoded_predictions, model_id=pt_model_name)\n",
    "# perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "# results = perplexity.compute(predictions=predictions, model_id='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_haikus[\"train\"].select(np.arange(len(tokenized_haikus[\"train\"]))[:2000])\n",
    "small_test_dataset = tokenized_haikus[\"test\"].select(np.arange(len(tokenized_haikus[\"test\"]))[:15])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm=False, return_tensors = \"pt\")\n",
    "\n",
    "pt_model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pt_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_web_data[\"train\"],\n",
    "    eval_dataset=tokenized_web_data['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.evaluate()\n",
    "pt_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate + Evaluate </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_input = tokenizer('( iced coffee =', return_tensors='pt').to(device)\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\"./result9\")\n",
    "\n",
    "trained_model.to(device)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    eos_token_id=trained_model.config.eos_token_id,\n",
    "    pad_token=trained_model.config.pad_token_id,\n",
    "    no_repeat_ngram_size=1,\n",
    "    do_sample = True,\n",
    ")\n",
    "\n",
    "generation_output = trained_model.generate(**generate_input, generation_config=generation_config)\n",
    "\n",
    "decoded_output = tokenizer.batch_decode(generation_output, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input = tokenizer('( iced coffee = ', return_tensors='pt').to(device)\n",
    "trained_model.to(device)\n",
    "pt_model.to(device)\n",
    "\n",
    "# trained_loss = trained_model(input_ids = eval_input[\"input_ids\"], labels = eval_input[\"input_ids\"]).loss\n",
    "# pt_loss = pt_model(input_ids = eval_input[\"input_ids\"], labels = eval_input[\"input_ids\"]).loss\n",
    "# trained_ppl = torch.exp(trained_loss)\n",
    "# pt_ppl = torch.exp(pt_loss)\n",
    "# print(trained_ppl)\n",
    "# print(pt_ppl)\n",
    "print(\"ppl: \", perplexity.compute(predictions=decoded_output, model_id=pt_model_name))\n",
    "print(\"ppl gpt: \", perplexity.compute(predictions=decoded_output, model_id=\"gpt2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_results = []\n",
    "for data in tokenized_haikus['test']:\n",
    "    data_tok = tokenizer(\"( \" + data['keywords'] + \" = \", return_tensors='pt').to(device)\n",
    "    our_output = trained_model.generate(**data_tok, generation_config=generation_config)\n",
    "    our_decoded_output = tokenizer.batch_decode(our_output, skip_special_tokens=True)\n",
    "    our_results.append(our_decoded_output[0])\n",
    "\n",
    "iced_coffee = tokenizer('( iced coffee = ', return_tensors='pt').to(device)\n",
    "iced_coffee = trained_model.generate(**iced_coffee, generation_config=generation_config)\n",
    "iced_coffee = tokenizer.batch_decode(iced_coffee, skip_special_tokens=True)\n",
    "\n",
    "snowman = tokenizer('( snowman = ', return_tensors='pt').to(device)\n",
    "snowman = trained_model.generate(**snowman, generation_config=generation_config)\n",
    "snowman = tokenizer.batch_decode(snowman, skip_special_tokens=True)\n",
    "\n",
    "### this was part of the dataset\n",
    "haiku = tokenizer('( haiku = ', return_tensors='pt').to(device)\n",
    "haiku = trained_model.generate(**haiku, generation_config=generation_config)\n",
    "haiku = tokenizer.batch_decode(haiku, skip_special_tokens=True)\n",
    "\n",
    "our_ppl = perplexity.compute(predictions=our_results, model_id=pt_model_name)['mean_perplexity']\n",
    "our_ppl_gpt2 = perplexity.compute(predictions=our_results, model_id=\"gpt2\")['mean_perplexity']\n",
    "\n",
    "print(\"ppl: \", our_ppl)\n",
    "print(\"ppl gpt: \", our_ppl_gpt2)\n",
    "print(\"iced_coffee: \", iced_coffee)\n",
    "print(\"snowman: \", snowman)\n",
    "print(\"haiku: \", haiku)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iced_coffee = tokenizer('iced coffee', return_tensors='pt').to(device)\n",
    "iced_coffee = trained_model.generate(**iced_coffee, generation_config=generation_config)\n",
    "print(tokenizer.batch_decode(iced_coffee, skip_special_tokens=True))\n",
    "\n",
    "snowman = tokenizer('snowman', return_tensors='pt').to(device)\n",
    "snowman = trained_model.generate(**snowman, generation_config=generation_config)\n",
    "print(tokenizer.batch_decode(snowman, skip_special_tokens=True))\n",
    "\n",
    "### this was part of the dataset\n",
    "our_chanting = tokenizer('our chanting', return_tensors='pt').to(device)\n",
    "our_chanting = trained_model.generate(**our_chanting, generation_config=generation_config)\n",
    "print(tokenizer.batch_decode(our_chanting, skip_special_tokens=True))\n",
    "\n",
    "print(clean_result(generator(\"( iced coffee = \")))\n",
    "print(clean_result(generator(\"( snowman = \")))\n",
    "print((generator(\"( haiku = \")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.117594,
   "end_time": "2023-10-06T19:44:58.522329",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-06T19:44:54.404735",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
